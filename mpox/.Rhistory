scale(X, scale = FALSE)
colMeans(scale(X, scale = FALSE))
scale(X, scale = FALSE) - X
tau = .2
X = scale(as.matrix(data2), scale = FALSE)
S = X[1,] %*% t(X[1,])
for(i in 2:100) {
S = S + X[i,] %*% t(X[i,])
}
S = S/100
Theta_MLE = solve(S)
E_hat = abs(Theta_MLE) > tau
E_hat
A
library(mvtnorm)
X = rmvnorm(3000, mean = rep(0, 3000))
solve(X)
data = read.csv(url("https://www.fhwa.dot.gov/bridge/nbi/2022/delimited/CA22.txt"), header = TRUE)
data
library(hypervolume)
# load in lat/lon data
data('quercus')
data_alba = subset(quercus, Species=="Quercus alba")[,c("Longitude","Latitude")]
data_rubra = subset(quercus, Species=="Quercus rubra")[,c("Longitude","Latitude")]
# get worldclim data from internet
climatelayers <- getData('worldclim', var='bio', res=10, path=tempdir())
# z-transform climate layers to make axes comparable
climatelayers_ss = climatelayers[[c(1,4,12,15)]]
for (i in 1:nlayers(climatelayers_ss))
{
climatelayers_ss[[i]] <- (climatelayers_ss[[i]] - cellStats(climatelayers_ss[[i]], 'mean')) / cellStats(climatelayers_ss[[i]], 'sd')
}
climatelayers_ss_cropped = crop(climatelayers_ss, extent(-150,-50,15,60))
# extract transformed climate values
climate_alba = extract(climatelayers_ss_cropped, data_alba)
climate_rubra = extract(climatelayers_ss_cropped, data_rubra)
library(raster)
library(maps)
data_alba = subset(quercus, Species=="Quercus alba")[,c("Longitude","Latitude")]
data_rubra = subset(quercus, Species=="Quercus rubra")[,c("Longitude","Latitude")]
# get worldclim data from internet
climatelayers <- getData('worldclim', var='bio', res=10, path=tempdir())
# z-transform climate layers to make axes comparable
climatelayers_ss = climatelayers[[c(1,4,12,15)]]
for (i in 1:nlayers(climatelayers_ss))
{
climatelayers_ss[[i]] <- (climatelayers_ss[[i]] - cellStats(climatelayers_ss[[i]], 'mean')) / cellStats(climatelayers_ss[[i]], 'sd')
}
head(climatelayers)
install.packages("terra")
install.packages("terra")
install.packages("geodata")
install.packages("corrplot")
install.packages("corrplot")
library("corrplot")
install.packages("ggplot2")
install.packages("mosaic")
install.packages("corrplot")
install.packages("corrplot")
install.packages("corrplot")
install.packages("corrplot")
6*(1/2)^5
pnorm(.8, .5, sqrt(.25/5), lower.tail = FALSE)
.224/sqrt(35/87 * (1 - 35/87) * (1/43 + 1/44))
138 + 169
307/(11034 + 11037)
-.003/sqrt(0.01390966 * (1 - 0.01390966) * (1/11034 + 1/11037))
2 * pnorm(-1.902767)
knitr::opts_chunk$set(echo = TRUE)
treatment = c(rep(0, 169), rep(1, 11037 - 169))
placebo = c(rep(0, 138), rep(1, 11034 - 138))
data = c(treatment, placebo)
treatment_identifier = c(rep(0, 11034), rep(1, 11037))
diffs = numeric(10000)
for(i in 1:10000) {
shuffle_i = sample(treatment_identifier)
diffs[i] = mean(data[shuffle_i == 0])-mean(data[shuffle_i == 1])
}
mean(abs(diffs) >= 0.003)
11037-169
11034-138
169/11037
138/11034
-.003/sqrt(0.01531213 * (1 - 0.01531213)/11037 + 0.0125068 * (1 - 0.0125068)/11034)
x = matrix(1:1000, nrow = 10)
t(x)
x $*$ t(x)
x %*% t(x)
eigen(x %*% t(x))
eigen(x %*% t(x))$values
eigen(x[,1:10] %*% t(x[,1:10]))$values
x[1:10]
x[,1:10]
x = matrix(1:100, nrow = 4)
eigen(x %*% t(x))$value
knitr::opts_chunk$set(echo = TRUE)
x = matrix(1:10, nrow = 2)
x[,1:2]
x
eigen(x%*%t(x))
eigen(x%*%t(x))$vectors
U = eigen(x%*%t(x))$vectors
U %*% t(U)
x = matrix(1:100, nrow = 20)
U $*$ t(U)
U %*% t(U)
x = matrix(1:100, nrow = 20)
U = eigen(x%*%t(x))$vectors
U %*% t(U)
x = matrix(1:100, nrow = 10)
x = matrix(1:100, nrow = 10)
U = eigen(x%*%t(x))$vectors
U %*% t(U)
eigen(U %*% t(U))
eigen(U %*% matrix(c(1, rep(0, 9), 0, 1, rep(0, 8), rep(0, 80)), nrow = 10) %*% t(U))
U %*% matrix(c(1, rep(0, 9), 0, 1, rep(0, 8), rep(0, 80)), nrow = 10) %*% t(U)
U %*% matrix(c(1, rep(0, 9), 0, 1, rep(0, 8), rep(0, 80)), nrow = 10) %*% t(U)
U[,1:2] %*% t(U[,1:2])
U[,1:2] %*% t(U[,1:2]) == U %*% matrix(c(1, rep(0, 9), 0, 1, rep(0, 8), rep(0, 80)), nrow = 10) %*% t(U)
eigen(U)
knitr::opts_chunk$set(echo = TRUE)
avg_poor = rep(0, 1601 + 2116)
not_poor = rep(1, 2318 + 136)
t.test(c(avg_poor, not_poor))
avg_poor = rep(0, 1601 + 2116)
not_poor = rep(1, 2318 + 136)
treatment = c(avg_poor, not_poor)
avg_poor_control = rep(0, 1665 + 2983)
not_poor_control = rep(1, 1186 + 92)
control = c(avg_poor_control, not_poor_control)
outcomes = c(treatment, control)
labels = c(rep(1, length(treatment)), rep(0, length(control)))
t.test(outcome ~ label)
avg_poor = rep(0, 1601 + 2116)
not_poor = rep(1, 2318 + 136)
treatment = c(avg_poor, not_poor)
avg_poor_control = rep(0, 1665 + 2983)
not_poor_control = rep(1, 1186 + 92)
control = c(avg_poor_control, not_poor_control)
outcomes = c(treatment, control)
labels = c(rep(1, length(treatment)), rep(0, length(control)))
t.test(outcomes ~ label)
avg_poor = rep(0, 1601 + 2116)
not_poor = rep(1, 2318 + 136)
treatment = c(avg_poor, not_poor)
avg_poor_control = rep(0, 1665 + 2983)
not_poor_control = rep(1, 1186 + 92)
control = c(avg_poor_control, not_poor_control)
outcomes = c(treatment, control)
labels = c(rep(1, length(treatment)), rep(0, length(control)))
t.test(outcomes ~ labels)
t.test(outcomes ~ labels, alternative = "greater")
t.test(outcomes ~ labels, alternative = "lesser")
t.test(outcomes ~ labels, alternative = "less")
t.test(outcomes ~ labels, alternative = "two.sided")
t.test(treatment)
t.test(control)
R.home()
install.packages("lme4")
install.packages("lme4")
install.packages("lme4")
install.packages("magrittr")
install.packages("readr")
rtime
library(lme4)
library(readr)
library(magrittr)
testdata <- read_csv("lmm_data.csv")
install.packages("Matrix")
install.packages("Matrix")
library(installr)
install.packages("installr")
library(installr)
updateR()
install.packages("lme4")
library(lme4)
library(readr)
library(magrittr)
testdata <- read_csv("lmm_data.csv")
install.packages("Matrix")
library(lme4)
install.packages("lme4")
install.packages("magrittr")
install.packages("readr")
R.home()
knitr::opts_chunk$set(echo = TRUE)
# Obtain the 2nd and 5th item in a vector
example_nums[c(2, 5)]
example_nums = 1:10
example_nums
# Obtain the 2nd and 5th item in a vector
example_nums[c(2, 5)]
knitr::opts_chunk$set(echo = TRUE)
nums = 1:100
nums[10]
nums2 = -10:10
nums2 = -10:10
nums2[c(3, 4, 5)]
nums2[nums2 < 0]
getwd()
AgeBMI = read.table("AgeBMI.txt", header = TRUE)
head(AgeBMI)
age = AgeBMI$Age
bmi = AgeBMI$BMI
mean(bmi)
age = AgeBMI$Age
bmi = AgeBMI$bmi
mean(bmi)
bmi_under_29 = bmi < 29
table(bmi_under_29)
knitr::opts_chunk$set(echo = TRUE)
# This makes sure that the random number generator produces the same outcome each time
set.seed(123)
num_samples = 100
# The number of people who have contracted chicken pox from a single sample of 100 people where the true probability of success is 0.85.
num_contracted = rbinom(1, num_samples, prob = 0.85)
num_contracted
# Fill in the following three variables based on the information given in the question
observed = 87/num_samples
expected = .9
SE = sqrt(.9 * (1 - .9)/num_samples)
# Code for calculation of z and one sided p value
z = (observed - expected)/SE
pval = pnorm(z)
# Print p value
pval
# This makes sure that the random number generator produces the same outcome each time
set.seed(123)
num_samples = 1000
# The number of people who have contracted chicken pox from a single sample of 100 people where the true probability of success is 0.85.
num_contracted = rbinom(1, num_samples, prob = 0.85)
num_contracted
# Fill in the following three variables based on the information given in the question
observed = 856/num_samples
expected = .9
SE = sqrt(.9 * (1 - .9)/num_samples)
# Code for calculation of z and one sided p value
z = (observed - expected)/SE
pval = pnorm(z)
# Print p value
pval
num_flips = 30
num_heads = 12
p = num_heads/num_flips
se = sqrt(p * (1 - p) / num_flips)
c(p-1.96*se, p+1.96*se)
num_flips = 30
num_heads = 21
p = num_heads/num_flips
se = sqrt(p * (1 - p) / num_flips)
c(p-1.96*se, p+1.96*se)
in_conf_int = numeric(10000)
for(i in 1:10000){
num_flips = 30
num_heads = rbinom(1, 30, prob = 0.5)
p = num_heads/num_flips
se = sqrt(p * (1 - p) / num_flips)
in_conf_int[i] = 0.5 >= (p-1.96*se) & 0.5 <= (p+1.96*se)
}
mean(in_conf_int)
barplot(table(in_conf_int)/10000, main = "Confidence Interval contains 0.5")
qt(97.5, 29)
qt(0.975, 29)
qt(0.025, 29)
c(65.02 - 2.04523 * 51.42, 65.02 + 2.04523 * 51.42)
c(65.02 - 2.04523 * 51.42/sqrt(30), 65.02 + 2.04523 * 51.42/sqrt(30))
letters
letters[0]
letters[0:11]
letters[1:11]
x = letters[0:11]
x[100]
x[-1]
x[0]
help("vapply")
obj4 <- seq(2, 10, 2)
fun4a <- function(x) {
x + 2
}
fun4bc <- function(x, y = 2) {
x^y
}
vapply(obj4, fun4a, obj4[1])
vapply(obj4, fun4a)
vapply(obj4, fun4a, 2)
vapply(obj4, fun4a, 1846)
vapply(obj4, fun4bc, numeric(1))
vapply(obj4, fun4bc, 1234)
vapply(obj4, fun4bc, 1, y=3)
vapply(obj4, fun4bc, 100, y=3)
fun4bc(obj4, y = 3)
x = [1, 2, 3, 4]
x = c(1, 2, 3, 4)
length(x)
length(x) <- 2
x
length(x)
length(x) <- 5
x
setwd("C:/Users/14842/Desktop/Point Processes/mpox")
set.seed(8147)
## load in data from mpox_cases_XY GPS_Sept13.xlsx
setwd("C:/Users/14842/Desktop/Point Processes/mpox")
x = read.table("NSK.txt",header=T)
x = read.table("NSK.txt",header=T)
## x is now 50 by 4.
onset1 = as.Date(x[,1],"%m/%d/%y")
sampdate1 = as.Date(x[,2],"%m/%d/%y")
xcoord1 = x[,3]
ycoord1 = x[,4]
interv1 = as.numeric(sampdate1-onset1)
mean(interv,na.rm=T) ## 8.27
hist(interv,nclass=40)
## If sampdate = NA, remove it.
n = nrow(x)
ind = c(1:n)[!is.na(sampdate1)]
onset2 = onset1[ind]
sampdate2 = sampdate1[ind]
xcoord2 = xcoord1[ind]
ycoord2 = ycoord1[ind]
interv2 = interv1[ind]
interv3 = interv2[!is.na(interv2)]
n = length(ycoord2)
## If onset = NA, sample from the distribution of sampdate - onset, to get
## estimated onset times.
for(i in 1:n){
if(is.na(onset2[i])){
onset2[i] = sampdate2[i] - sample(interv3)[1]
}
}
onset3 = julian(onset2)
min(onset3) ## 19692.
onset4 = onset3 - (min(onset3)-1)
z = list()
z$t = sort(onset4) ## I sorted them here.
z$lon = c(runif(z$n))
z$lat = c(runif(z$n))
z$n = length(z$t)
T = max(z$t) + 1
system("R CMD SHLIB mysg.c")
dyn.load("mysg.so")
system("R CMD SHLIB mysg.c")
find_rtools()
system("R CMD SHLIB mysg.c")
dyn.load("mysg.so")
system("R CMD SHLIB mysg.c")
dyn.load("mysg.dll")
X1 = 1
Y1 = 1
M0 = 3.5
m3 = function(x) signif(x,3)
## Construct a list, wbin, where wbin[[17]] = c() if bin 17 is empty, and
## if wbin[[17]] = c(1,2,10), then points 1,2,and 10 are in bin 17.
## I will have 10 x 10 x 10 = 1000 bins.
wbin = list()
for(i in 1:1000) wbin[[i]] = c(0)
for(m in 1:z$n) {
gridindex = 10*10*floor(z$t[m]*10/T)+
10*floor(z$lon[m]*10/X1)+ceiling(z$lat[m]*10/Y1)
wbin[[gridindex]] = c(wbin[[gridindex]],m)
}
## Construct a list, wbin, where wbin[[17]] = c() if bin 17 is empty, and
## if wbin[[17]] = c(1,2,10), then points 1,2,and 10 are in bin 17.
## I will have 10 x 10 x 10 = 1000 bins.
wbin = list()
for(i in 1:1000) wbin[[i]] = c(0)
for(m in 1:z$n) {
gridindex = 10*10*floor(z$t[m]*10/T)+
10*floor(z$lon[m]*10/X1)+ceiling(z$lat[m]*10/Y1)
wbin[[gridindex]] = c(wbin[[gridindex]],m)
}
z$n
set.seed(8147)
## load in data from mpox_cases_XY GPS_Sept13.xlsx
setwd("C:/Users/14842/Desktop/Point Processes/mpox")
x = read.table("NSK.txt",header=T)
## x is now 50 by 4.
onset1 = as.Date(x[,1],"%m/%d/%y")
sampdate1 = as.Date(x[,2],"%m/%d/%y")
xcoord1 = x[,3]
ycoord1 = x[,4]
interv1 = as.numeric(sampdate1-onset1)
mean(interv,na.rm=T) ## 8.27
hist(interv,nclass=40)
## If sampdate = NA, remove it.
n = nrow(x)
ind = c(1:n)[!is.na(sampdate1)]
onset2 = onset1[ind]
sampdate2 = sampdate1[ind]
xcoord2 = xcoord1[ind]
ycoord2 = ycoord1[ind]
interv2 = interv1[ind]
interv3 = interv2[!is.na(interv2)]
n = length(ycoord2)
## If onset = NA, sample from the distribution of sampdate - onset, to get
## estimated onset times.
for(i in 1:n){
if(is.na(onset2[i])){
onset2[i] = sampdate2[i] - sample(interv3)[1]
}
}
onset3 = julian(onset2)
min(onset3) ## 19692.
onset4 = onset3 - (min(onset3)-1)
z = list()
z$t = sort(onset4) ## I sorted them here.
z$lon = c(runif(z$n))
z$lat = c(runif(z$n))
z$n = length(z$t)
z$lon = c(runif(z$n))
z$lat = c(runif(z$n))
z$n = length(z$t)
z = list()
z$t = sort(onset4) ## I sorted them here.
z$lon = c(runif(z$n))
z$n = length(z$t)
z$lon = c(runif(z$n))
z$lat = c(runif(z$n))
T = max(z$t) + 1
system("R CMD SHLIB mysg.c")
dyn.load("mysg.dll")
X1 = 1
Y1 = 1
M0 = 3.5
m3 = function(x) signif(x,3)
## Construct a list, wbin, where wbin[[17]] = c() if bin 17 is empty, and
## if wbin[[17]] = c(1,2,10), then points 1,2,and 10 are in bin 17.
## I will have 10 x 10 x 10 = 1000 bins.
wbin = list()
for(i in 1:1000) wbin[[i]] = c(0)
for(m in 1:z$n) {
gridindex = 10*10*floor(z$t[m]*10/T)+
10*floor(z$lon[m]*10/X1)+ceiling(z$lat[m]*10/Y1)
wbin[[gridindex]] = c(wbin[[gridindex]],m)
}
for(i in 1:1000) wbin[[i]] = wbin[[i]][-1]
sumsqstoyan = function(theta,draw=0){
mu = theta[1]; K = theta[2]; tmean = theta[3]; tsd = theta[4]
cat("\n mu = ",m3(mu),", K = ",m3(K),", tmean = ",m3(tmean),", tsd = ",m3(tsd),".\n")
if(min(mu,K,tmean,tsd)<0.000000001) return(99999)
if(K>.99999) return(99999)
if(draw){
r = seq(0,3,length=100)
t = alpha/pi * exp(-alpha * r^2)
lines(r,t,col="orange",lty=2)
}
const = K
b = T*X1*Y1/10/10/10
mysum = rep(b,1000)
for(i in 1:1000){ ## i is the bin index.
if(length(wbin[[i]]) > .5){
mysum[i] = 0
for(j in wbin[[i]]){ ## j is the index of a point in bin i.
gkj = 0
if(j>1) for(k in 1:(j-1)){ ## k are indices of previous points.
# r2 = (z$lon[j]-z$lon[k])^2+(z$lat[j]-z$lat[k])^2
gkj = gkj + dnorm(z$t[j]-z$t[k],mean=tmean,sd=tsd)
}
lambdaj = mu/X1/Y1 + const*gkj
if(lambdaj < 0){
cat("lambda ",j," is less than 0.")
return(99999)
}
mysum[i] = mysum[i] + 1/lambdaj
}
}
}
if(draw) lines(r,t,col="white",lty=2)
sum((mysum-b)^2)
}
sumsqstoyan(c(1,.9,7,2))
theta1 = c(3,.9,7,2)
b1 = optim(theta1,sumsqstoyan)
b2 = optim(b1$par,sumsqstoyan,hessian=T)
theta2 = b2$par
sqrt(diag(solve(b2$hess))) ## for SEs
b1
b2
## plot the fit.
par(mfrow=c(1,1))
r = seq(0,20,length=100)
## s = theta0$beta * exp(-theta0$beta * r)
## plot(r,s,col="green",xlab="t",ylab="g(t)",type="l")
t = dnorm(r,mean=theta2[3],sd=theta2[4])
plot(r,t,col="blue",type="l",xlab="t (days)",ylab="estimated g(t)for NSK (cases/day)")
expxy = function(n,m,theta){
## exponential triggering in space. f(r) = alpha/pi exp(-alpha r^2).
## Here the density does not depend on magnitude of the mainshock.
## To see that this is a density,
## ∫f(x,y)dxdy = ∫f(r)rdrdø = 2π∫f(r)rdr
## = 2alpha ∫ exp(-alpha r^2) r dr = -exp(-alpha r^2) ,r=0to∞, = 0+1, for alpha>0.
v = rexp(n,rate=theta$alpha)
dist1 = sqrt(v)
thet1 = runif(n)*2*pi
x = cos(thet1)*dist1
y = sin(thet1)*dist1
cbind(x,y)
}
library(MASS)
g4 = expxy(100000,1,theta=list(alpha=theta2[3]))
g5 = kde2d(g4[,1],g4[,2],lims=c(-.3,.3,-.3,.3))
image(g5,main="estimated planar triggering function g(x,y) for Equateur")
contour(g5,add=T)
kde2d(x$HZ_POINT_X, x$HZ_POINT_Y)
plot(x$HZ_POINT_X, x$HZ_POINT_Y)
x = read.table("NSK.txt",header=T)
hist(interv,nclass=40)
interv1 = as.numeric(sampdate1-onset1)
mean(interv,na.rm=T) ## 8.27
hist(interv,nclass=40)
mean(interv1,na.rm=T) ## 8.27
hist(interv1,nclass=40)
